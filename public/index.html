<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
  <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="theme-color" content="#000000" />
  <meta name="description" content="Web site created using create-react-app" />
  <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
  <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
  <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
  <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
  <title>React App</title>
  <script>
    window.createAudioMeter = function (audioContext, clipLevel, averaging, clipLag) {
      var processor = audioContext.createScriptProcessor(512);
      processor.onaudioprocess = window.volumeAudioProcess;
      processor.clipping = false;
      processor.lastClip = 0;
      processor.volume = 0;
      processor.clipLevel = clipLevel || 0.98;
      processor.averaging = averaging || 0.95;
      processor.clipLag = clipLag || 750;

      // this will have no effect, since we don't copy the input to the output,
      // but works around a current Chrome bug.
      processor.connect(audioContext.destination);

      processor.checkClipping =
        function () {
          if (!this.clipping)
            return false;
          if ((this.lastClip + this.clipLag) < window.performance.now())
            this.clipping = false;
          return this.clipping;
        };

      processor.shutdown =
        function () {
          this.disconnect();
          this.onaudioprocess = null;
        };

      return processor;
    }

    window.volumeAudioProcess = function (event) {
      var buf = event.inputBuffer.getChannelData(0);
      var bufLength = buf.length;
      var sum = 0;
      var x;

      // Do a root-mean-square on the samples: sum up the squares...
      for (var i = 0; i < bufLength; i++) {
        x = buf[i];
        if (Math.abs(x) >= this.clipLevel) {
          this.clipping = true;
          this.lastClip = window.performance.now();
        }
        sum += x * x;
      }

      // ... then take the square root of the sum.
      var rms = Math.sqrt(sum / bufLength);

      // Now smooth this out with the averaging factor applied
      // to the previous sample - take the max here because we
      // want "fast attack, slow release."
      this.volume = Math.max(rms, this.volume * this.averaging);
    }


    var audioContext = null;
    var meter = null;
    var canvasContext = null;
    var WIDTH = 500;
    var HEIGHT = 50;
    var rafID = null;

    window.onload = function () {

      // grab our canvas
      // canvasContext = document.getElementById("meter").getContext("2d");

      // monkeypatch Web Audio
      window.AudioContext = window.AudioContext || window.webkitAudioContext;

      // grab an audio context
      audioContext = new AudioContext();

      // Attempt to get audio input
      try {
        // monkeypatch getUserMedia
        navigator.getUserMedia =
          navigator.getUserMedia ||
          navigator.webkitGetUserMedia ||
          navigator.mozGetUserMedia;

        // ask for an audio input
        navigator.getUserMedia({
          "audio": {
            "mandatory": {
              "googEchoCancellation": "false",
              "googAutoGainControl": "false",
              "googNoiseSuppression": "false",
              "googHighpassFilter": "false"
            },
            "optional": []
          },
        }, gotStream, didntGetStream);
      } catch (e) {
        alert('getUserMedia threw exception :' + e);
      }

    }


    function didntGetStream() {
      alert('Stream generation failed.');
    }

    var mediaStreamSource = null;

    function gotStream(stream) {
      // Create an AudioNode from the stream.
      mediaStreamSource = audioContext.createMediaStreamSource(stream);

      // Create a new volume meter and connect it.
      window.meter = window.createAudioMeter(audioContext);
      mediaStreamSource.connect(meter);

      // kick off the visual updating
      // drawLoop();
    }

    function drawLoop(time) {
      // clear the background
      canvasContext.clearRect(0, 0, WIDTH, HEIGHT);

      // check if we're currently clipping
      if (meter.checkClipping())
        canvasContext.fillStyle = "red";
      else
        canvasContext.fillStyle = "green";

      // draw a bar based on the current volume
      canvasContext.fillRect(0, 0, meter.volume * WIDTH * 1.4, HEIGHT);

      // set up the next visual callback
      rafID = window.requestAnimationFrame(drawLoop);
    }
  </script>
</head>

<body>
  <noscript>You need to enable JavaScript to run this app.</noscript>
  <div id="root"></div>
  <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
</body>

</html>